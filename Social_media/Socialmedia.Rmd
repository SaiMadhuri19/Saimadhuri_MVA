---
title: "Socialmedia"
author: "Sai Madhuri"
date: "2024-03-24"
output: html_document
---

```{r}
library(readxl)
sm <- read_excel("/Users/user/Downloads/social_media_cleaned.xlsx")
str(sm)
```
#Dependent Variable: How you felt the entire week?
#Independent Variables:Instagram,LinkedIn,SnapChat,Twitter,Whatsapp,youtube,OTT,Reddit

```{r}
sm1 <- sm[, 2:9]
sm1

```

```{r}
#load necessary libraries
library(magrittr)
library(NbClust)
library(cluster)
library(factoextra)
```
# Cluster Analysis
```{r}
#Hierarchical Clustering- Dendrogram
sm_scaled <- scale(sm1)
dist_matrix <- dist(sm_scaled)

#Clustering Single
hc <- hclust(dist_matrix,method = "single")
fviz_dend(hc)

```
```{r}
#Default Clustering

hc <- hclust(dist_matrix)
plot(hc, hang = -1, cex = 0.6, main = "Dendrogram for Hierarchical Clustering")

```
```{r}
#Average Clustering

hc <- hclust(dist_matrix,method = "average")
plot(hc, hang = -1, cex = 0.6, main = "Dendrogram for Hierarchical Clustering")

```

#By observing the above dendrogram's k=2 clusters will be sufficient.This is confirmed further with D index graphical representation.


```{r}
num_clusters <- 2
clusters <- cutree(hc, k = num_clusters)

# Membership for each cluster
table(clusters)

```

```{r}

# Principal Components
pca_result <- prcomp(sm1,scale=TRUE)
pca_result


```
```{r}
#Non-Hierarchical Clustering(k-means)
num_clusters <- 2
kmeans_model <- kmeans(sm1, centers = num_clusters)

# Membership for each cluster
table(kmeans_model$cluster)
```

```{r}
# Visualize cluster and membership using first two Principal Components
fviz_cluster(list(data = pca_result$x[, 1:2], cluster = kmeans_model$cluster))
```

```{r}
# Visualize cluster centers for k-means
fviz_cluster(kmeans_model, data = sm_scaled, geom = "point", frame.type = "convex", 
             pointsize = 2, fill = "white", main = "K-means Cluster Centers")
```

```{r}
# Visualize cluster and membership using first two Principal Components for k-means
pca_result <- prcomp(sm_scaled, scale = TRUE)
fviz_cluster(kmeans_model, data = pca_result$x[, 1:2], geom = "point", 
             pointsize = 2, fill = "white", main = "K-means Clustering Result (PCA)")
```

```{r}
# Calculate silhouette information for k-means clustering
sil <- silhouette(kmeans_model$cluster, dist(sm_scaled))

# Visualize the silhouette plot for k-means clustering
fviz_silhouette(sil, main = "Silhouette Plot for K-means Clustering")
```

```{r}
#optimal cluster method/visualization
res.nbclust <- sm1 %>% scale() %>% NbClust(distance = "euclidean", min.nc = 2, max.nc = 10, method = "complete", index ="all") 
```

#From cluster analysis I am able to recognize users whose social media usage patterns are similar to mine.

# PCA

```{r}
#Get the correlations between the variables 

cor(sm1, use = "complete.obs")
```
```{r}
#Computing Principal Components
social_pca <- prcomp(sm1,scale=TRUE)
social_pca

```

```{r}
summary(social_pca)
```
```{r}
eigen_social<- social_pca$sdev^2
eigen_social
```

#From PCA variate representation of each PC, It's evident that PC1 and PC2 add arround 50% of the to total variance.

#Screeplot
```{r}
plot(eigen_social, xlab = "Component number", ylab = "Component variance", type = "l", main = "Scree diagram")
```
```{r}
plot(log(eigen_social), xlab = "Component number", ylab = "Component variance", type = "l", main = "Scree diagram")
```
```{r}
library(FactoMineR)
res.pca <- PCA(sm_scaled, graph = FALSE)
fviz_eig(res.pca, addlabels = TRUE)

```

###From the screeplot elbow it is benefial to consider PC1,PC2,PC3,PC4,PC5,PC6 as it covers 93% of total variance.

#Visualization using PC's
```{r}
library(FactoMineR)
library("factoextra")
res.pca <- PCA(sm1, graph = FALSE)
fviz_pca_var(res.pca, col.var = "black")
```

#From the above I can tell that most of my classmates usage timings of the apps  Instagram,Whatsapp/Wechat, LinkedIn, Youtube and snapchat are similar. Most probably, twitter and reddit are not used by me and my classmates.



# Factor Analysis

```{r}
# load library for factor analysis
library(ggplot2)
library(psych)

```

#Decide how many Factors are ideal for your dataset?
```{r}
fa.parallel(sm1)
```
#Parallel analysis suggests that the number of factors =  0  and the number of components =  0


#Explain the output for your factor model?
```{r}
fit.pc <- principal(sm1, nfactors=2, rotate="varimax")
fit.pc
```

#High absolute values (close to 1) indicate a strong relationship between the variable and the factor.
#h2 explains how much variance of the variables are explained by the factors.
#u2 indicates the amount of variance not explained by the factors
Principal Components Analysis
Call: principal(r = sm1, nfactors = 2, rotate = "varimax")
Standardized loadings (pattern matrix) based upon correlation matrix

                       RC1  RC2
SS loadings           2.27 1.80
Proportion Var        0.25 0.20
Cumulative Var        0.25 0.45
Proportion Explained  0.56 0.44
Cumulative Proportion 0.56 1.00

Mean item complexity =  1.3
Test of the hypothesis that 2 components are sufficient.

The root mean square of the residuals (RMSR) is  0.14 
 with the empirical chi square  29.01  with prob <  0.066 
 

```{r}
round(fit.pc$values, 3)
```


```{r}
fit.pc$loadings
```
```{r}
# Communalities
fit.pc$communality

```
```{r}
# Rotated factor scores, Notice the columns ordering: RC1, RC2
fit.pc
fit.pc$scores

```
```{r}
fa.plot(fit.pc) # See Correlations within Factors

```
Show the columns that go into each factor?
```{r}
fa.diagram(fit.pc) # Visualize the relationship

```


Perform some visualizations using the factors

```{r}
#very simple structure visualization
vss(sm1)

```
Very Simple Structure
Call: vss(x = sm1)
VSS complexity 1 achieves a maximimum of 0.61  with  6  factors
VSS complexity 2 achieves a maximimum of 0.78  with  7  factors

The Velicer MAP achieves a minimum of 0.06  with  1  factors 
BIC achieves a minimum of  -53.17  with  1  factors
Sample Size adjusted BIC achieves a minimum of  1.47  with  5  factors

Statistics by number of factors 

```{r}
# Computing Correlation Matrix
corrm.social <- cor(sm1)
corrm.social

```
```{r}
plot(corrm.social)

```
```{r}
social_pca <- prcomp(sm1, scale=TRUE)
summary(social_pca)

```


```{r}
plot(social_pca)

```


#Biplot Visualization
```{r}
biplot(fit.pc)

```



#I feel factor analysis is not beneficial for the social media data because I observed that we are missing the most part of the uniqueness of these apps by including factors and we are able to capture only a small portion of variances by using factors.
#And parallel analysis screeplot indicated that the ideal number of factors for the social media data is zero.
#From the component analysis we got similar results to PCA, where the apps like Instagram, whatsapp/wechat, LinkedIn, Youtube, Snapchat usages are a bit similar and high compared to OTT, Twitter and Reddit.


* Key takeaways from my analysis
  * Interestingly I found users having same pattern social media usage as mine are not more energetic the entire week like me.(3/5). This suggests that we need to reduce our social media usage to become more productive.
  * Instagram, Whatsapp/Wechat, LinkedIn, Youtube, Snapchat are the most popular apps used by the students.
  

### Multiple Regression

```{r}
sm2 <- sm[, 2:10]
sm2

```

### 1.Model Development
```{r}

# Performing multiple regression on the dataset
fit <- lm(sm2$`How you felt the entire week?` ~ sm2$Instagram+ sm2$LinkedIn + sm2$SnapChat + sm2$Twitter+ sm2$`Whatsapp/Wechat`+ sm2$youtube + sm2$OTT + sm2$Reddit , data=sm2)
#show the results
summary(fit)
```

#From the above summary we got p-value 0.7774 which indicates the coefficient of the predictor variable associated with this p-value is not statistically significant.

```{r}
coefficients(fit)
```
###From the above we get information about the dependent variable in equation form y=b0+ b1x1 + b2x2+...+bnxn where intercept b0=2.68, and cofficients b1=-0.033,....

```{r}
fitted(fit)
```
### Residual Analysis

```{r}
library(GGally)
```
```{r}
ggpairs(data=sm2, title="Social-Media")
```

```{r}
plot(fit, which=1) # Residuals vs Fitted
plot(fit, which=2) # Normal Q-Q plot

```

```{r}
residuals <- residuals(fit)
```

```{r}
#Plot residuals against fitted values to check for homoscedasticity
plot_resid_fitted <- ggplot() +
  geom_point(aes(x = fitted(fit), y = residuals)) +
  geom_hline(yintercept = 0, linetype = "dashed", color = "red") +
  labs(x = "Fitted Values", y = "Residuals",
       title = "Residuals vs Fitted Values Plot") +
  theme_minimal()
print(plot_resid_fitted)

```
#The residual vs. fitted plot is a tool used to evaluate the assumptions and adequacy of a regression model. It helps to identify whether the model adequately captures the underlying relationships in the data or if there are issues that need to be addressed.
#The plot shows a pattern of points around zero, the model is likely not appropriate.

### Prediction

```{r}
predict.lm(fit, data.frame(Instagram=8, LinkedIn=5, SnapChat=4, Twitter=4,
    Whatsapp=4, youtube=8, OTT=3, Reddit=4 ))
```
### Model Accuracy
```{r}
#Make predictions using the model
predicted <- predict(fit, newdata = sm2)
```

```{r}
#Calculating RMSE by taking the square root of the mean of the squared differences between the actual values and the predicted values (predicted)
rmse <- sqrt(mean((sm2$`How you felt the entire week?` - predicted)^2))
rmse
```

#Low RMSE(0.613) between 0 and 1 indicates that the models predictions are quite accurate, with small deviations from the actual values.

### Some Visualizations

```{r}
library(car)
#Nonlinearity
# component + residual plot
crPlots(fit)
```
```{r}
# plot studentized residuals vs. fitted values
library(car)
spreadLevelPlot(fit)
```





# Logistic Regression

```{r}
#Load required Libraries
library(dplyr)
library(tidyr)
library(MASS)
```


### 1.Model Development
```{r}
# Create binary outcome variable: high mpg (1) vs. low mpg (0)
sm <- sm %>%
  mutate(new_prediction = ifelse(sm$`How you felt the entire week?` > median(sm$`How you felt the entire week?`), 1, 0))
```


```{r}
# Performing logistic regression on the dataset
logistic_simple <- glm(new_prediction ~ sm$Instagram + sm$LinkedIn + sm$SnapChat + sm$Twitter + sm$`Whatsapp/Wechat` + sm$youtube + sm$OTT + sm$Reddit , data = sm, family = binomial)
```
#Building the model by considering the entire week energy in binary format as new_prediction and building the model by considering the effect of instagram,LinkedIn, SnapChat, Twitter, Whatsapp, Youtube, OTT and reddit


### 2.Model Acceptance
```{r}
summary(logistic_simple)
```

#From the above it is evident that the model is not acceptable since the independent variables are not contributing significantly to the variation in the dependent variable.

### 3.Residual Analysis
```{r}
residuals(logistic_simple)
```
```{r}
plot(logistic_simple, which = 1)
```
#There is a fixed pattern in the residuals vs fitted plot which means that the selected independent variables will not explain the dependent variable well.

### 4.Prediction

```{r}
# Make predictions on the same dataset
predicted_values <- predict(logistic_simple, type = "response")
predicted_values
```
```{r}
# Convert predicted probabilities to binary predictions (0 or 1) based on a threshold 0.5
predicted_class <- ifelse(predicted_values > 0.5, 1, 0)
predicted_class
```

### 5.Accuracy

#we can compute accuracy by comparing predicted values with the original data

```{r}
original <- sm$new_prediction  # Assuming new_prediction contains binary response variable (0 or 1)
accuracy <- mean(predicted_class == original)
print(accuracy)


```

#Accuracy Provides an overall measure of model performance. An accuracy of 0.714 indicates that the logistic regression model correctly predicted the outcome (the value of new_prediction) approximately 71.4% correctly.


### Visualizations
```{r}
# Install pROC package (only need to run once)
#install.packages("pROC")

# Load the pROC package
library(pROC)
```


```{r}
length(sm$new_prediction)
length(predicted_class)
```

```{r}
# Compute ROC curve for the logistic regression model
roc_curve <- roc(original, predicted_class)

# Plot ROC curve
plot(roc_curve, legacy.axes = TRUE, main = "ROC Curve for Logistic Regression Model")
```


```{r}
roc(original,predicted_class,plot=TRUE, legacy.axes=TRUE, xlab="False Positive Percentage", ylab="True Postive Percentage", col="#377eb8", lwd=4, percent=TRUE, print.auc=TRUE, partial.auc=c(100, 90), auc.polygon = TRUE, auc.polygon.col = "#377eb822", print.auc.x=45)
```


#ROC curves are typically used to evaluate the performance of binary classification models, where the predicted classes represent binary outcomes (e.g., positive/negative, yes/no). Here the dependent variable is not binary so the model is not appropriate.




# Linear Discriminant Analysis
```{r}
colnames(sm)[9]<- "Reddit"
colnames(sm)[6]<- "wa"
colnames(sm)[10] <- "weeksenergy"
str(sm)
```

```{r}
# Splitting the dataset into 75% training and 25% test sets
smp_size_raw <- floor(0.75 * nrow(sm))
train_ind_raw <- sample(nrow(sm), size = smp_size_raw)
train_raw.df <- sm[train_ind_raw, ]
test_raw.df <- sm[-train_ind_raw, ]
```

### 1.Model Development
```{r}
lda_model <- lda(train_raw.df$weeksenergy ~ Instagram + LinkedIn + SnapChat + Twitter + wa  + youtube + OTT + Reddit, data = train_raw.df)


lda_model

```

#Prior probability shows the class distribution of each class in the training data.The data is classified into 4 groups(based on the peoples happiness measured on the scale of 5 (2,3,4,5 are the categories)) and LD1 having few significant coefficients.Among the groups 2,3,4,5 "3" is the most predominant group.
#Coefficients of Linear Discriminants: The coefficients of linear discriminants represent the weights assigned to each predictor variable in the discriminant function.


### 2.Model Acceptance
```{r}
summary(lda_model)
```
#with significant coefficients and no obvious issues with class imbalance, the LDA model is likely acceptable. Here only LinkedIn variable plays a vital role. LDA model is moderately acceptable.


```{r}

plot(lda_model)
```

### 3.Residual Analysis
```{r}
residuals(lda_model)
```

#LDA does not inherently produce residuals

### 4.Prediction
```{r}
prediction <- predict(lda_model, test_raw.df)
prediction
```
### 5.Accuracy

```{r}
# Predict on the test set
predicted_classes <- predict(lda_model, test_raw.df)$class

# Create a confusion matrix to understand misclassifications
confusion_matrix <- table(predicted_classes, test_raw.df$weeksenergy)
confusion_matrix 

accuracy <- sum(predicted_classes == test_raw.df$weeksenergy) / nrow(test_raw.df)
accuracy 

```

#Acuuracy is very low suggesting LDA is not suitable to classify the dataset
```{r}
str(train_raw.df)
```
```{r}
library(klaR)
attach(train_raw.df)
train_raw.df$weeksenergy <- factor(train_raw.df$weeksenergy)
partimat( weeksenergy ~Instagram+LinkedIn+SnapChat+Twitter+youtube+OTT+Reddit, data=train_raw.df, method="lda")
```

